{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-24T12:41:25.648Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Basic Imports \n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "wines_df = pd.read_csv(\"../data/winequality.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# tf doesn't like spaces in col names so I replace them with _ \n",
    "\n",
    "new_col_list = []\n",
    "for col_name in wines_df.columns:\n",
    "    new_col_names = col_name.replace(\" \", \"_\")\n",
    "    new_col_list.append(new_col_names)\n",
    "wines_df.columns = new_col_list\n",
    "\n",
    "\n",
    "# Change the type of the index values from [0....1598] to [wine_1....wine_1599]\n",
    "\n",
    "wines_df.index += 1 # add 1 to index values to start wine specification from wine_1 rather than wine_0\n",
    "\n",
    "index_as_string = wines_df.index.astype('str')\n",
    "\n",
    "wines_df.index = 'wine_' + index_as_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset to training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T12:10:06.890884Z",
     "start_time": "2020-04-24T12:10:06.842664Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# shuffle the data of the wines_df\n",
    "\n",
    "wines_df = wines_df.sample(frac=1) \n",
    "\n",
    "# almost 70% training , 15% validation, 15% test set\n",
    "\n",
    "intermediate_set, valid_set = train_test_split(wines_df, test_size=0.15) \n",
    "train_set, test_set = train_test_split(intermediate_set, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_set.pop('quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_y = valid_set.pop('quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = test_set.pop('quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# sad\n",
    "train_set has 1155 examples\n",
    "for batch_size equal to 33 we have batch_count = 1155/33 = 35\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1155/33\n",
    "\n",
    "train_set.shape\n",
    "\n",
    "valid_set.shape\n",
    "\n",
    "test_set.shape\n",
    "\n",
    "valid_set has 240 examples\n",
    "\n",
    "for batch_size = 20 we have 240/ 20 = 12 number of batches\n",
    "\n",
    "test_set has 204 examples\n",
    "for batch_size = 17 we have 204/17 = 12 number of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "train_batch_size = 33\n",
    "valid_batch_size = 20\n",
    "test_batch_size = 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# When to use df.copy\n",
    "\n",
    "\n",
    "# This will change the original df because of df[0:1]\n",
    "df = pd.DataFrame({'x': [1,2,3]})\n",
    "df_sub = df[0:2]\n",
    "display(df, df_sub)\n",
    "df_sub.x = -10\n",
    "print(\"The original df changed as well\")\n",
    "display(df)\n",
    "print(\"As we changed dfsub\")\n",
    "display(df_sub)\n",
    "\n",
    "\n",
    "# Instead If I create a copy I wont have this problem\n",
    "\n",
    "print(\"We create the dataframes again \")\n",
    "\n",
    "df = pd.DataFrame({'x': [1,2,3]})\n",
    "df.sub = df[0:2].copy()\n",
    "display(df, df_sub)\n",
    "df_sub.x = -10\n",
    "print(\"Will the original df change now?\")\n",
    "display(df)\n",
    "print(\"Comparing it to dfsub hoping they are different\")\n",
    "display(df_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T12:10:09.636072Z",
     "start_time": "2020-04-24T12:10:09.625344Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# creating the input function for training, validation, prediction\n",
    "def input_fn(dataframe, labels, batch_size):\n",
    "    \n",
    "    '''\n",
    "    dataframe = train_set or valid_ set or test_set\n",
    "    \n",
    "    labels = train_y or valid_set or test_set\n",
    "    \n",
    "    batch_size = 33 for training or 20 for validation or 17 for test_set   \n",
    "    '''\n",
    "    \n",
    "    # normalization_1\n",
    "    df = dataframe.copy()\n",
    "    min_values = df.min(axis=0)\n",
    "    max_values = df.max(axis=0)\n",
    "    max_minus_min = max_values - min_values\n",
    "    df = df - min_values\n",
    "    df = df.div(max_minus_min)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((df.values, labels.values))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    \n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = input_fn(dataframe=valid_set, labels=valid_y, batch_size=valid_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Doulepse\n",
    "for row in data:\n",
    "\n",
    "    feat , labels = row\n",
    "    \n",
    "    # build model graph \n",
    "    inputs = tf.keras.Input(shape=(11,))\n",
    "    dense_1 = tf.keras.layers.Dense(units=20, input_shape=(33,11), activation='relu', dtype='float32')(inputs)\n",
    "    dense_2 = tf.keras.layers.Dense(units=10, input_shape=(33,20), activation=\"relu\", dtype='float32')(dense_1)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=dense_2)\n",
    "    \n",
    "    \n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    # model's output is the logits for every batch\n",
    "    \n",
    "    logit = model(feat)\n",
    "    probs = tf.keras.activations.softmax(logit)\n",
    "    class_predictions = tf.argmax(probs, axis=1, output_type=tf.int64)\n",
    "\n",
    "\n",
    "    # check the first data point\n",
    "    print(f\"The logits of each class in the first data point are \\n {logit[0,:]}\")   \n",
    "    print(\"\\n\")\n",
    "    print(f\"The probabilities of each class in the first data point are \\n {probs[0,:]}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"The index of the most probable class for the first data point is \\n {class_predictions[0]}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(labels, tf.cast(probs, tf.float32)))\n",
    "    # var_list = [dense_1.trainable_weights, dense_2.trainable_weights]\n",
    "    \n",
    "    \n",
    "    print(\"The loss is {}\".format(loss))\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# model_fn without feature_columns\n",
    "\n",
    "def model_fn(features, labels, mode, params):\n",
    "    \n",
    "    \n",
    "    \n",
    "    # building the model graph\n",
    "    inputs = tf.keras.Input(shape=(11,))\n",
    "    dense_1 = tf.keras.layers.Dense(20, activation='relu', name='dense_1', dtype=tf.float32)(inputs)\n",
    "    dense_2 = tf.keras.layers.Dense(10, activation='relu', name='dense_2', dtype=tf.float32)(dense_1)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=dense_2)\n",
    "    \n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        print(f\"The model's variables are: {model.trainable_variables}\")\n",
    "    \n",
    "        logits = model(features)\n",
    "        probs = tf.keras.activations.softmax(logits)\n",
    "        class_index = tf.argmax(probs, axis=1 , output_type=tf.int64)\n",
    "    \n",
    "   \n",
    "        def my_acc(labels,probs):\n",
    "            acc_metric = tf.keras.metrics.SparseCategoricalAccuracy(name='my_acc')\n",
    "            acc_metric.update_state(y_true=labels, y_pred=probs)\n",
    "            return {'acc': acc_metric}\n",
    "    \n",
    "    \n",
    "        print(\"\\n\",\"Hello user\")\n",
    "        print(\"\\n\")\n",
    "        print(f\"The logits of each class in the first data point are \\n {logits[0,:]}\")   \n",
    "        print(\"\\n\")\n",
    "        print(f\"The probabilities of each class in the first data point are \\n {probs[0,:]}\")\n",
    "        print(\"\\n\")\n",
    "        print(f\"The index of the most probable class for the first data point is \\n {class_index[0]}\")\n",
    "        print(\"\\n\")\n",
    "        print(f\"The model's variables are: {model.trainable_variables}\")\n",
    "        \n",
    "        @tf.function\n",
    "        def loss():\n",
    "            return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(labels,probs))\n",
    "    \n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "\n",
    "            \n",
    "            opt = tf.keras.optimizers.Adam()\n",
    "            #train_op = opt.minimize(loss,model.trainable_variables) doesn't work\n",
    "            gradients = tape.gradient(loss(), model.trainable_variables)\n",
    "            train_op = opt.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            return tf.estimator.EstimatorSpec(mode, loss=loss(), train_op = train_op)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#          opt = tf.keras.optimizers.Adam()\n",
    "#          with tf.GradientTape() as tape:\n",
    "#              dl_dw = tape.gradient(loss(), var_list)\n",
    "#              train_op = opt.apply_gradients(zip(dl_dw, var_list))\n",
    "        \n",
    "#         return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "\n",
    "        elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "            #sca = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "            #update_op = sca.update_state(y_true=labels, y_pred=probs)\n",
    "            #update_op = tf.keras.metrics.SparseCategoricalAccuracy.update_state(labels,probs)\n",
    "            metrics = my_acc(labels,probs)\n",
    "            return tf.estimator.EstimatorSpec(mode,loss=loss(), eval_metric_ops=metrics)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        elif mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            predictions = {\n",
    "                'top_class_index' : tf.argmax(probs),\n",
    "                'probs_of_classes' : probs,\n",
    "                'logits_of_classes' : logits\n",
    "            }\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_estimator = tf.estimator.Estimator(model_fn = model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_estimator.train(input_fn=lambda:input_fn(train_set, train_y, train_batch_size), steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_estimator.evaluate(input_fn=lambda:input_fn(valid_set, valid_y, valid_batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = my_estimator.predict(input_fn=lambda:input_fn(test_set, test_y, test_batch_size), yield_single_examples=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of function minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Example of minimizing a function f(x) = x^2-10x+25 with .minimize\n",
    "\n",
    "\n",
    "x = tf.Variable(0,dtype=tf.float32, trainable=True)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def f_x():\n",
    "    return x**2 -10*x + 25\n",
    "\n",
    "\n",
    "\n",
    "for _ in range(200):\n",
    "    print([x.numpy(), f_x().numpy()])\n",
    "    opt = tf.keras.optimizers.SGD(0.01).minimize(f_x,[x])\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T21:13:20.133146Z",
     "start_time": "2020-04-22T21:13:20.016453Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Example of minimizing a function f(x) = x^2-10x+25 with GradientTape\n",
    "\n",
    "\n",
    "x = tf.Variable([0], dtype=tf.float32, trainable=True)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def f_x():\n",
    "    return x**2 -10*x +25\n",
    "\n",
    "opt = tf.keras.optimizers.SGD(0.01)\n",
    "for _ in range(200):\n",
    "    print([x[0].numpy(), f_x().numpy()])\n",
    "    with tf.GradientTape() as tape:\n",
    "        df_dx = tape.gradient(f_x(),x)\n",
    "        opt.apply_gradients(zip([df_dx],[x]))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.constant([0,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T21:13:25.376131Z",
     "start_time": "2020-04-22T21:13:25.372354Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = tf.constant([[4, 2, 0],[1,5,3],[3,1,1]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.metrics.sparse_categorical_accuracy(y_true,y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# model_fn with feature_columns \n",
    "\n",
    "def my_model_fn(features, labels, mode, params):\n",
    "    \n",
    "    # extract those from params dict\n",
    "    feature_names = ['fixed_acidity','volatile_acidity', 'citric_acid', 'residual_sugar', 'chlorides', 'free_sulfur_dioxide', 'total_sulfur_dioxide','density','pH','sulphates','alcohol']   \n",
    "    \n",
    "    feature_columns =[]\n",
    "    feature_layer_inputs = {}  # dictionary that maps feature names to tf.keras.Input layers\n",
    "    for names in feature_names:\n",
    "        feature_columns.append(tf.feature_column.numeric_column(names))\n",
    "        feature_layer_inputs[names] = tf.keras.Input(dtype=tf.dtypes.float32, shape=(,11), batch_size=33, name=names)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    # model's architecture\n",
    "    inputs = tf.\n",
    "    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)(feature_layer_inputs)\n",
    "    dense_1 = tf.keras.layers.Dense(20, activation='relu',name='dense_1',dtype=tf.float32)(feature_layer)\n",
    "    dense_2 = tf.keras.layers.Dense(10, activation='relu', name='dense_2', dtype=tf.float32)(dense_1)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=dense_2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        print(f\"The model's variables are: {model.trainable_variables}\")\n",
    "        logits = model(features)\n",
    "        probs = tf.keras.activations.softmax(logits)\n",
    "        class_index = tf.argmax(probs, axis=1 , output_type=tf.int64)\n",
    "    \n",
    "   \n",
    "        def my_acc(labels,probs):\n",
    "            acc_metric = tf.keras.metrics.SparseCategoricalAccuracy(name='my_acc')\n",
    "            acc_metric.update_state(y_true=labels, y_pred=probs)\n",
    "            return {'acc': acc_metric}\n",
    "    \n",
    "    \n",
    "        print(\"\\n\",\"Hello user\")\n",
    "        print(\"\\n\")\n",
    "        print(f\"The logits of each class in the first data point are \\n {logits[0,:]}\")   \n",
    "        print(\"\\n\")\n",
    "        print(f\"The probabilities of each class in the first data point are \\n {probs[0,:]}\")\n",
    "        print(\"\\n\")\n",
    "        print(f\"The index of the most probable class for the first data point is \\n {class_index[0]}\")\n",
    "        print(\"\\n\")\n",
    "        print(f\"The model's variables are: {model.trainable_variables}\")\n",
    "        \n",
    "        @tf.function\n",
    "        def loss():\n",
    "            return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(labels,probs))\n",
    "    \n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "\n",
    "            \n",
    "            opt = tf.keras.optimizers.Adam()\n",
    "            #train_op = opt.minimize(loss,model.trainable_variables) doesn't work\n",
    "            gradients = tape.gradient(loss(), model.trainable_variables)\n",
    "            train_op = opt.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            return tf.estimator.EstimatorSpec(mode, loss=loss(), train_op = train_op)\n",
    "        \n",
    "        \n",
    "        elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "            #sca = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "            #update_op = sca.update_state(y_true=labels, y_pred=probs)\n",
    "            #update_op = tf.keras.metrics.SparseCategoricalAccuracy.update_state(labels,probs)\n",
    "            metrics = my_acc(labels,probs)\n",
    "            return tf.estimator.EstimatorSpec(mode,loss=loss(), eval_metric_ops=metrics)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        elif mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            predictions = {\n",
    "                'top_class_index' : tf.argmax(probs),\n",
    "                'probs_of_classes' : probs,\n",
    "                'logits_of_classes' : logits\n",
    "            }\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_zveboyc\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_zveboyc', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:Estimator's model_fn (<function my_model_fn at 0x7f14f43c8320>) includes params argument, but params are not passed to Estimator.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "(33, 11)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input tensors to a Model must come from `tf.keras.Input`. Received: Tensor(\"concat:0\", shape=(33, 11), dtype=float32) (missing previous layer metadata).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-184-674f4d460d0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmy_estimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEstimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_model_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmy_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf_env/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_env/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1162\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_env/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1192\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m       estimator_spec = self._call_model_fn(\n\u001b[0;32m-> 1194\u001b[0;31m           features, labels, ModeKeys.TRAIN, self.config)\n\u001b[0m\u001b[1;32m   1195\u001b[0m       \u001b[0mglobal_step_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n",
      "\u001b[0;32m~/anaconda3/envs/tf_env/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-183-d66a17bc05c2>\u001b[0m in \u001b[0;36mmy_model_fn\u001b[0;34m(features, labels, mode, params)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mdense_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dense_1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mdense_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dense_2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdense_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0m_keras_api_gauge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;31m# initializing _distribution_strategy here since it is possible to call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         'inputs' in kwargs and 'outputs' in kwargs):\n\u001b[1;32m    168\u001b[0m       \u001b[0;31m# Graph network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m       \u001b[0;31m# Subclassed network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_env/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs, name, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_graph_inputs_and_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;31m# A Network does not create weights of its own, thus it is already\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_validate_graph_inputs_and_outputs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m                          \u001b[0;34m'must come from `tf.keras.Input`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m                          \u001b[0;34m'Received: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1328\u001b[0;31m                          ' (missing previous layer metadata).')\n\u001b[0m\u001b[1;32m   1329\u001b[0m       \u001b[0;31m# Check that x is an input tensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input tensors to a Model must come from `tf.keras.Input`. Received: Tensor(\"concat:0\", shape=(33, 11), dtype=float32) (missing previous layer metadata)."
     ]
    }
   ],
   "source": [
    "my_estimator = tf.estimator.Estimator(model_fn = my_model_fn)\n",
    "\n",
    "my_estimator.train(input_fn=lambda:input_fn(train_set, train_y, train_batch_size), steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
