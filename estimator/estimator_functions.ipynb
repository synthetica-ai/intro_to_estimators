{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-24T12:41:25.648Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Basic Imports\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-24T12:41:25.648Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "\n",
    "\n",
    "wines_df = pd.read_csv(\"../data/winequality.csv\")\n",
    "\n",
    "###########################################\n",
    "\n",
    "# tf doesn't like spaces in column names so I replace them with _ \n",
    "\n",
    "new_col_list = []\n",
    "for col_name in wines_df.columns:\n",
    "    new_col_names = col_name.replace(\" \", \"_\")\n",
    "    new_col_list.append(new_col_names)\n",
    "wines_df.columns = new_col_list\n",
    "\n",
    "\n",
    "\n",
    "###########################################\n",
    "\n",
    "# Change the type of the index values from [0....1598] to [wine_1....wine_1599]\n",
    "\n",
    "wines_df.index += 1 # add 1 to index values to start wine specification from wine_1 rather than wine_0\n",
    "\n",
    "index_as_string = wines_df.index.astype('str')\n",
    "\n",
    "wines_df.index = 'wine_' + index_as_string\n",
    "\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset to training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T12:10:06.890884Z",
     "start_time": "2020-04-24T12:10:06.842664Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# shuffle the data of the wines_df\n",
    "\n",
    "wines_df = wines_df.sample(frac=1) \n",
    "\n",
    "# almost 70% training , 15% validation, 15% test set\n",
    "\n",
    "intermediate_set, valid_set = train_test_split(wines_df, test_size=0.15) \n",
    "train_set, test_set = train_test_split(intermediate_set, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_set.pop('quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_y = valid_set.pop('quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = test_set.pop('quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_creator_v2(df=test_set,samples=1):\n",
    "    '''\n",
    "    A function that creates a json file containing the test samples in appropriate format for tf serving\n",
    "    '''\n",
    "    import json\n",
    "    \n",
    "    samples = df.head(samples)\n",
    "    samples_dict = samples.to_dict(orient='records')\n",
    "    for elements in samples_dict:\n",
    "        for key,values in elements.items():\n",
    "            elements[key] = [values]\n",
    "\n",
    "        \n",
    "    json_dict = {}\n",
    "    json_dict[\"instances\"] = samples_dict\n",
    "    with open('results.json','w') as f:\n",
    "        json.dump(json_dict,f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_creator_v2(samples=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "train_batch_size = 33\n",
    "valid_batch_size = 20\n",
    "test_batch_size = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'learning_rate': 0.001,\n",
    "         'feature_names':['fixed_acidity','volatile_acidity', 'citric_acid', 'residual_sugar', 'chlorides', 'free_sulfur_dioxide', 'total_sulfur_dioxide','density','pH','sulphates','alcohol'],\n",
    "         'train_batch_size' : 33,\n",
    "         'valid_batch_size' : 20,\n",
    "         'test_batch_size' : 17,\n",
    "         'number_of_hidden_units': [200,180,10], \n",
    "         'num_epochs' : 5,\n",
    "         }\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.001,\n",
       " 'feature_names': ['fixed_acidity',\n",
       "  'volatile_acidity',\n",
       "  'citric_acid',\n",
       "  'residual_sugar',\n",
       "  'chlorides',\n",
       "  'free_sulfur_dioxide',\n",
       "  'total_sulfur_dioxide',\n",
       "  'density',\n",
       "  'pH',\n",
       "  'sulphates',\n",
       "  'alcohol'],\n",
       " 'train_batch_size': 33,\n",
       " 'valid_batch_size': 20,\n",
       " 'test_batch_size': 17,\n",
       " 'number_of_hidden_units': [200, 180, 10],\n",
       " 'num_epochs': 5,\n",
       " 'fixed_acidity': [15.9, 4.6],\n",
       " 'volatile_acidity': [1.58, 0.12],\n",
       " 'citric_acid': [1.0, 0.0],\n",
       " 'residual_sugar': [15.4, 0.9],\n",
       " 'chlorides': [0.61, 0.012],\n",
       " 'free_sulfur_dioxide': [72.0, 1.0],\n",
       " 'total_sulfur_dioxide': [289.0, 6.0],\n",
       " 'density': [1.00369, 0.99007],\n",
       " 'pH': [4.01, 2.74],\n",
       " 'sulphates': [2.0, 0.33],\n",
       " 'alcohol': [14.9, 8.4]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# When to use df.copy\n",
    "\n",
    "\n",
    "# This will change the original df because of df[0:1]\n",
    "df = pd.DataFrame({'x': [1,2,3]})\n",
    "df_sub = df[0:2]\n",
    "display(df, df_sub)\n",
    "df_sub.x = -10\n",
    "print(\"The original df changed as well\")\n",
    "display(df)\n",
    "print(\"As we changed dfsub\")\n",
    "display(df_sub)\n",
    "\n",
    "\n",
    "# Instead If I create a copy I wont have this problem\n",
    "\n",
    "print(\"We create the dataframes again \")\n",
    "\n",
    "df = pd.DataFrame({'x': [1,2,3]})\n",
    "df.sub = df[0:2].copy()\n",
    "display(df, df_sub)\n",
    "df_sub.x = -10\n",
    "print(\"Will the original df change now?\")\n",
    "display(df)\n",
    "print(\"Comparing it to dfsub hoping they are different\")\n",
    "display(df_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# for serving input function \n",
    "\n",
    "def max_min_finder(train_set, params):\n",
    "    my_dict = {}\n",
    "    for columns in params['feature_names']:\n",
    "        max_value = train_set[columns].max()\n",
    "        min_value = train_set[columns].min()\n",
    "        my_dict[columns] = [max_value,min_value]\n",
    "    return my_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.update(max_min_finder(train_set,params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.001, 'feature_names': ['fixed_acidity', 'volatile_acidity', 'citric_acid', 'residual_sugar', 'chlorides', 'free_sulfur_dioxide', 'total_sulfur_dioxide', 'density', 'pH', 'sulphates', 'alcohol'], 'train_batch_size': 33, 'valid_batch_size': 20, 'test_batch_size': 17, 'number_of_hidden_units': [200, 180, 10], 'num_epochs': 5, 'fixed_acidity': [15.9, 4.6], 'volatile_acidity': [1.58, 0.12], 'citric_acid': [1.0, 0.0], 'residual_sugar': [15.4, 0.9], 'chlorides': [0.61, 0.012], 'free_sulfur_dioxide': [72.0, 1.0], 'total_sulfur_dioxide': [289.0, 6.0], 'density': [1.00369, 0.99007], 'pH': [4.01, 2.74], 'sulphates': [2.0, 0.33], 'alcohol': [14.9, 8.4]}\n"
     ]
    }
   ],
   "source": [
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.001,\n",
       " 'feature_names': ['fixed_acidity',\n",
       "  'volatile_acidity',\n",
       "  'citric_acid',\n",
       "  'residual_sugar',\n",
       "  'chlorides',\n",
       "  'free_sulfur_dioxide',\n",
       "  'total_sulfur_dioxide',\n",
       "  'density',\n",
       "  'pH',\n",
       "  'sulphates',\n",
       "  'alcohol'],\n",
       " 'train_batch_size': 33,\n",
       " 'valid_batch_size': 20,\n",
       " 'test_batch_size': 17,\n",
       " 'number_of_hidden_units': [200, 180, 10],\n",
       " 'num_epochs': 5}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wines_df.copy()\n",
    "min_values = df.min(axis=0)\n",
    "max_values = df.max(axis=0)\n",
    "max_minus_min = max_values - min_values\n",
    "df = df - min_values\n",
    "df = df.div(max_minus_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixed_acidity           True\n",
      "volatile_acidity        True\n",
      "citric_acid             True\n",
      "residual_sugar          True\n",
      "chlorides               True\n",
      "free_sulfur_dioxide     True\n",
      "total_sulfur_dioxide    True\n",
      "density                 True\n",
      "pH                      True\n",
      "sulphates               True\n",
      "alcohol                 True\n",
      "quality                 True\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print((df.any() >=0) & (df.any()<=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T12:10:09.636072Z",
     "start_time": "2020-04-24T12:10:09.625344Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# creating the input function for training, validation, prediction\n",
    "def my_input_fn(dataframe, labels, batch_size, train):\n",
    "    \n",
    "    '''\n",
    "    dataframe = train_set or valid_ set or test_set\n",
    "    \n",
    "    labels = train_y or valid_set or test_set\n",
    "    \n",
    "    batch_size = 33 for training or 20 for validation or 17 for test_set   \n",
    "    '''\n",
    "    \n",
    "    # normalization\n",
    "    df = dataframe.copy()\n",
    "    min_values = df.min(axis=0)\n",
    "    max_values = df.max(axis=0)\n",
    "    max_minus_min = max_values - min_values\n",
    "    df = df - min_values\n",
    "    df = df.div(max_minus_min) \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((df.values, labels.values))\n",
    "    if train:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = my_input_fn(dataframe=train_set, labels=train_y, batch_size=train_batch_size, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.Dataset.shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Doulepse\n",
    "logits = []\n",
    "probabilities = []\n",
    "predicts = []\n",
    "lbls = []\n",
    "\n",
    "\n",
    "for row in data.take(3):\n",
    "\n",
    "    feat , labels = row\n",
    "    \n",
    "    # build model graph \n",
    "    inputs = tf.keras.Input(shape=(11,))\n",
    "    dense_1 = tf.keras.layers.Dense(units=20, input_shape=(33,11), activation='relu', dtype='float32')(inputs)\n",
    "    dense_2 = tf.keras.layers.Dense(units=10, input_shape=(33,20), activation=\"softmax\", dtype='float32')(dense_1)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=dense_2)\n",
    "    \n",
    "    \n",
    "\n",
    "    lbls.append(labels)\n",
    "    \n",
    "    # model's output is the logits for every batch\n",
    "    \n",
    "    \n",
    "    \n",
    "    logit = model(feat)\n",
    "    \n",
    "    \n",
    "    print(logit)\n",
    "    aha = []\n",
    "    for j in logit:\n",
    "        aha.append(sum(j))\n",
    "    \n",
    "    print(aha)\n",
    "    print(len(aha))\n",
    "    \n",
    "    break\n",
    "    logits.append(logit)\n",
    "    probs = tf.keras.activations.softmax(logit)\n",
    "    probabilities.append(probs)\n",
    "    preds = tf.argmax(probs, axis=1, output_type=tf.int64)\n",
    "    predicts.append(preds)\n",
    "\n",
    "    \n",
    "    \n",
    "    #print(class_preds_per_point)\n",
    "\n",
    "    # check the first data point\n",
    "#     print(f\"The logits of each class in the first data point are \\n {logit[0]}\")   \n",
    "#     print(\"\\n\")\n",
    "#     print(f\"The probabilities of each class in the first data point are \\n {probs[0,:]}\")\n",
    "#     print(\"\\n\")\n",
    "#     print(f\"The index of the most probable class for the first data point is \\n {preds[0]}\")\n",
    "#     print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(labels, tf.cast(probs, tf.float32)))\n",
    "    # var_list = [dense_1.trainable_weights, dense_2.trainable_weights]\n",
    "    \n",
    "    \n",
    "#     print(\"The loss is {}\".format(loss))\n",
    "#     print(probs)\n",
    "#     print(\"\\n\")\n",
    "#     print(preds)\n",
    "#     print(\"\\n\")\n",
    "#     print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Estimator Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model_fn without feature_columns\n",
    "\n",
    "def model_fn(features, labels, mode, params=params):\n",
    "    \n",
    "    \n",
    "    \n",
    "    # building the model graph\n",
    "    inputs = tf.keras.Input(shape=(11,))\n",
    "    dense_1 = tf.keras.layers.Dense(20, activation='relu', name='dense_1', dtype=tf.float32)(inputs)\n",
    "    dense_2 = tf.keras.layers.Dense(15, activation='relu', name='dense_2', dtype=tf.float32)(dense_1)\n",
    "    dense_3 = tf.keras.layers.Dense(10,  activation='softmax', name='dense_3', dtype=tf.float32)(dense_2)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=dense_3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def loss():\n",
    "        scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "        return tf.reduce_mean(scce(labels,probs))\n",
    "    \n",
    "    \n",
    "    def my_acc(labels,probs):    \n",
    "        '''\n",
    "        SparseCategoricalAccuracy\n",
    "\n",
    "        Calculates how often predictions match integer labels\n",
    "\n",
    "        Example\n",
    "\n",
    "        label = [3,4]\n",
    "\n",
    "        probs = [[0.3, 0.1, 0.2, 0.4, 0] , [0.4, 0.2, 0.1, 0, 0.3] match_indx_0] \n",
    "\n",
    "        In the first array of probs we have a match between the index with the highest prob (3,0.4)\n",
    "        and the label 3.\n",
    "\n",
    "        In the second array of probs we dont have a much since the index with the highest prob is 0\n",
    "\n",
    "        Therefore sca = # matches / # total labels = 1/2 = 0.50\n",
    "\n",
    "        '''\n",
    "\n",
    "        acc_metric = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
    "        acc_metric.update_state(y_true=labels, y_pred=probs)\n",
    "        return {'SparseCategoricalAcc': acc_metric}\n",
    "    \n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        probs = model(features)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            optimizer = tf.keras.optimizers.Adam()\n",
    "            optimizer.iterations = tf.compat.v1.train.get_global_step()\n",
    "            gradients = tape.gradient(loss(), model.trainable_variables)\n",
    "            train_op = optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            return tf.estimator.EstimatorSpec(mode, loss=loss(), train_op = train_op)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        metrics = my_acc(labels,probs)\n",
    "        return tf.estimator.EstimatorSpec(mode,loss=loss(), eval_metric_ops=metrics)\n",
    "\n",
    "\n",
    "\n",
    "    elif mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        index_preds = tf.argmax(probs,axis=1,output_type=tf.int64)\n",
    "        predictions = {\n",
    "            'class_index_predictions' : index_preds,\n",
    "            'probs_of_classes' : probs\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "configurations = tf.estimator.RunConfig(save_summary_steps=35, save_checkpoints_steps=35, keep_checkpoint_max=2)\n",
    "my_estimator = tf.estimator.Estimator(model_fn = model_fn, model_dir='../model/checkpoints', params=params, config=configurations)\n",
    "\n",
    "for i in range(params['num_epochs']):\n",
    "    my_estimator.train(input_fn=lambda:my_input_fn(train_set, train_y, params['train_batch_size'], train=True), steps=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "my_estimator.evaluate(input_fn=lambda:input_fn(valid_set, valid_y, params['valid_batch_size'], train=False), steps=12, checkpoint_path='../model/checkpoints/model.ckpt-525')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preds = my_estimator.predict(input_fn=lambda:input_fn(test_set, test_y, params['test_batch_size'], train=False), yield_single_examples=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### train_and_evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In order to use the \n",
    "\n",
    "**tf.estimator.train_and_evaluate(\n",
    "    estimator, train_spec, eval_spec\n",
    ")**\n",
    "\n",
    "method we need to supply the specifications for training *train_spec* and evaluation *eval_spec* accordingly.\n",
    "\n",
    "\n",
    "* tf.estimator.TrainSpec(\n",
    "    input_fn, max_steps=None, hooks=None)\n",
    "    \n",
    "* tf.estimator.EvalSpec(\n",
    "    input_fn, steps=100, name=None, hooks=None, exporters=None,\n",
    "    start_delay_secs=120, throttle_secs=600\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_spec = tf.estimator.TrainSpec(input_fn=lambda:my_input_fn(train_set, train_y, train_batch_size, train=True), max_steps=35*NUM_EPOCHS)\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=lambda:my_input_fn(valid_set, valid_y, valid_batch_size, train=False), steps=12, start_delay_secs=60, throttle_secs=120 )\n",
    "\n",
    "for i in range(params['num_epochs']):\n",
    "    tf.estimator.train_and_evaluate(my_estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Examples of function minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Example of minimizing a function f(x) = x^2-10x+25 with .minimize\n",
    "\n",
    "\n",
    "x = tf.Variable(0,dtype=tf.float32, trainable=True)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def f_x():\n",
    "    return x**2 -10*x + 25\n",
    "\n",
    "\n",
    "\n",
    "for _ in range(200):\n",
    "    print([x.numpy(), f_x().numpy()])\n",
    "    opt = tf.keras.optimizers.SGD(0.01).minimize(f_x,[x])\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T21:13:20.133146Z",
     "start_time": "2020-04-22T21:13:20.016453Z"
    },
    "code_folding": [],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Example of minimizing a function f(x) = x^2-10x+25 with GradientTape\n",
    "\n",
    "\n",
    "x = tf.Variable([0], dtype=tf.float32, trainable=True)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def f_x():\n",
    "    return x**2 -10*x +25\n",
    "\n",
    "opt = tf.keras.optimizers.SGD(0.01)\n",
    "for _ in range(200):\n",
    "    print([x[0].numpy(), f_x().numpy()])\n",
    "    with tf.GradientTape() as tape:\n",
    "        df_dx = tape.gradient(f_x(),x)\n",
    "        opt.apply_gradients(zip([df_dx],[x]))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_true = tf.constant([0,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T21:13:25.376131Z",
     "start_time": "2020-04-22T21:13:25.372354Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred = tf.constant([[4, 2, 0],[1,5,3],[3,1,1]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tf.keras.metrics.sparse_categorical_accuracy(y_true,y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable([[1,2,3],\n",
    "                 [4,5,6]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "for row in data:\n",
    "    \n",
    "    feat, label = row\n",
    "    \n",
    "    \n",
    "    feature_names = ['fixed_acidity','volatile_acidity', 'citric_acid', 'residual_sugar', 'chlorides', 'free_sulfur_dioxide', 'total_sulfur_dioxide','density','pH','sulphates','alcohol']   \n",
    "    #feats = tf.split(feat,11,axis=1)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    INPUT_FEATURE = 'x'\n",
    "    feature_columns =[tf.feature_column.numeric_column(INPUT_FEATURE, shape=(11,))]\n",
    "    \n",
    "    feature_layer_inputs = {}  # dictionary that maps feature names to tf.keras.Input layers\n",
    "    #for names in feature_names:\n",
    "        #feature_columns.append(tf.feature_column.numeric_column(names))\n",
    "    feature_layer_inputs[INPUT_FEATURE] = tf.keras.Input(shape=(11,), dtype=tf.float32)\n",
    "        \n",
    "    \n",
    "    \n",
    "    feature_layer = tf.keras.layers.DenseFeatures(feature_columns,dtype=tf.float32)(feature_layer_inputs)\n",
    "    dense_1 = tf.keras.layers.Dense(20, activation='relu',name='dense_1',dtype=tf.float32)(feature_layer)\n",
    "    dense_2 = tf.keras.layers.Dense(10, activation='relu', name='dense_2', dtype=tf.float32)(dense_1)\n",
    "    model = tf.keras.Model(inputs=feature_layer_inputs, outputs=dense_2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    logits = model(feat)\n",
    "    probs = tf.keras.activations.softmax(logits)\n",
    "    class_index = tf.argmax(probs, axis=1 , output_type=tf.int64)\n",
    "    \n",
    "    loss = tf.reduce_sum(tf.keras.losses.sparse_categorical_crossentropy(label,probs))\n",
    "    #print(loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# model_fn with feature_columns \n",
    "\n",
    "def my_model_fn(features, labels, mode, params):\n",
    "    \n",
    "\n",
    "    # extract those from params dict\n",
    "#     feature_names = ['fixed_acidity','volatile_acidity', 'citric_acid', 'residual_sugar', 'chlorides', 'free_sulfur_dioxide', 'total_sulfur_dioxide','density','pH','sulphates','alcohol']   \n",
    "    \n",
    "    \n",
    "#     feats = tf.split(features,11,axis=1)\n",
    "    \n",
    "#     feature_columns =[]\n",
    "#     feature_layer_inputs = {}  # dictionary that maps feature names to tf.keras.Input layers\n",
    "#     for names in feature_names:\n",
    "#         feature_columns.append(tf.feature_column.numeric_column(names))\n",
    "#         feature_layer_inputs[names] = tf.keras.Input(dtype=tf.dtypes.float64, shape=(1,), name=names)\n",
    "    \n",
    "    \n",
    "    input_feature = 'x'\n",
    "    feature_columns =[tf.feature_column.numeric_column(input_feature, shape=(11,))]    \n",
    "    feature_layer_inputs = { input_feature: tf.keras.Input(shape=(11,), dtype=tf.float32)} \n",
    "    \n",
    "    \n",
    "    # model's architecture\n",
    "    \n",
    "    feature_layer = tf.keras.layers.DenseFeatures(feature_columns,dtype=tf.float64)\n",
    "    feature_layer_outputs = feature_layer(feature_layer_inputs)\n",
    "    dense_1 = tf.keras.layers.Dense(220, activation='relu',name='dense_1',dtype=tf.float64)(feature_layer_outputs)\n",
    "    dense_2 = tf.keras.layers.Dense(350, activation='relu', name='dense_2', dtype=tf.float64)(dense_1)\n",
    "    dense_3 = tf.keras.layers.Dense(350, activation='relu', name='dense_3', dtype=tf.float64)(dense_2)\n",
    "    dense_4 = tf.keras.layers.Dense(10, activation='softmax', name='dense_4', dtype=tf.float64)(dense_3)\n",
    "    model = tf.keras.Model(inputs=[v for v in feature_layer_inputs.values()], outputs=dense_4)\n",
    "    \n",
    "    \n",
    "    def my_acc(labels,probs):\n",
    "        acc_metric = tf.keras.metrics.SparseCategoricalAccuracy(name='my_acc')\n",
    "        acc_metric.update_state(y_true=labels, y_pred=probs)\n",
    "        return {'acc': acc_metric}\n",
    "    \n",
    "    \n",
    "    def loss():\n",
    "        return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(labels,probs))\n",
    "    \n",
    "\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        tape.watch(model.trainable_variables)\n",
    "        \n",
    "        probs = model(features)\n",
    "        class_index = tf.argmax(probs, axis=1 , output_type=tf.int64)\n",
    "    \n",
    "   \n",
    "        \n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            opt = tf.keras.optimizers.Adam()\n",
    "            opt.iterations = tf.compat.v1.train.get_global_step()\n",
    "            gradients = tape.gradient(loss(), model.trainable_variables)\n",
    "            train_op = opt.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            return tf.estimator.EstimatorSpec(mode, loss=loss(), train_op = train_op)\n",
    "        \n",
    "        \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        metrics = my_acc(labels,probs)\n",
    "        return tf.estimator.EstimatorSpec(mode,loss=loss(), eval_metric_ops=metrics)\n",
    "\n",
    "\n",
    "\n",
    "    elif mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'top_class_index' : class_index,\n",
    "            'probs_of_classes' : probs\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '../model/my_second_estimator', '_tf_random_seed': None, '_save_summary_steps': 35, '_save_checkpoints_steps': 35, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 1, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "configurations = tf.estimator.RunConfig(save_summary_steps=35, save_checkpoints_steps=35, keep_checkpoint_max=1)\n",
    "my_second_estimator = tf.estimator.Estimator(model_fn = my_model_fn, model_dir='../model/my_second_estimator', params=params, config=configurations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jpriest/anaconda3/envs/tf_env/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/jpriest/anaconda3/envs/tf_env/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ../model/my_second_estimator/model.ckpt-35\n",
      "WARNING:tensorflow:From /home/jpriest/anaconda3/envs/tf_env/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 35 into ../model/my_second_estimator/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.0034117736484713, step = 35\n",
      "INFO:tensorflow:Saving checkpoints for 70 into ../model/my_second_estimator/model.ckpt.\n",
      "WARNING:tensorflow:From /home/jpriest/anaconda3/envs/tf_env/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:Loss for final step: 0.8390346226619476.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.estimator.EstimatorV2 at 0x7f7a31bfac50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_second_estimator.train(lambda:my_input_fn(dataframe=train_set, labels=train_y, batch_size=train_batch_size, train=True), steps=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 35 or save_checkpoints_secs None.\n",
      "WARNING:tensorflow:From /home/jpriest/anaconda3/envs/tf_env/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/jpriest/anaconda3/envs/tf_env/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ../model/my_second_estimator/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.2993057595780244, step = 0\n",
      "INFO:tensorflow:Saving checkpoints for 35 into ../model/my_second_estimator/model.ckpt.\n",
      "WARNING:tensorflow:From /home/jpriest/anaconda3/envs/tf_env/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-08-23T01:02:19Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ../model/my_second_estimator/model.ckpt-35\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/12]\n",
      "INFO:tensorflow:Evaluation [2/12]\n",
      "INFO:tensorflow:Evaluation [3/12]\n",
      "INFO:tensorflow:Evaluation [4/12]\n",
      "INFO:tensorflow:Evaluation [5/12]\n",
      "INFO:tensorflow:Evaluation [6/12]\n",
      "INFO:tensorflow:Evaluation [7/12]\n",
      "INFO:tensorflow:Evaluation [8/12]\n",
      "INFO:tensorflow:Evaluation [9/12]\n",
      "INFO:tensorflow:Evaluation [10/12]\n",
      "INFO:tensorflow:Evaluation [11/12]\n",
      "INFO:tensorflow:Evaluation [12/12]\n",
      "INFO:tensorflow:Inference Time : 0.30803s\n",
      "INFO:tensorflow:Finished evaluation at 2020-08-23-01:02:20\n",
      "INFO:tensorflow:Saving dict for global step 35: acc = 0.5708333, global_step = 35, loss = 1.0481333\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 35: ../model/my_second_estimator/model.ckpt-35\n",
      "INFO:tensorflow:Loss for final step: 1.291743804374703.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'acc': 0.5708333, 'loss': 1.0481333, 'global_step': 35}, [])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_spec = tf.estimator.TrainSpec(input_fn=lambda:my_input_fn(train_set, train_y, train_batch_size, train=True), max_steps=35*params['num_epochs'])\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=lambda:my_input_fn(valid_set, valid_y, valid_batch_size, train=False), steps=12)\n",
    "\n",
    "tf.estimator.train_and_evaluate(my_second_estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.001,\n",
       " 'feature_names': ['fixed_acidity',\n",
       "  'volatile_acidity',\n",
       "  'citric_acid',\n",
       "  'residual_sugar',\n",
       "  'chlorides',\n",
       "  'free_sulfur_dioxide',\n",
       "  'total_sulfur_dioxide',\n",
       "  'density',\n",
       "  'pH',\n",
       "  'sulphates',\n",
       "  'alcohol'],\n",
       " 'train_batch_size': 33,\n",
       " 'valid_batch_size': 20,\n",
       " 'test_batch_size': 17,\n",
       " 'number_of_hidden_units': [200, 180, 10],\n",
       " 'num_epochs': 5,\n",
       " 'fixed_acidity': [15.6, 4.7],\n",
       " 'volatile_acidity': [1.58, 0.12],\n",
       " 'citric_acid': [1.0, 0.0],\n",
       " 'residual_sugar': [15.5, 0.9],\n",
       " 'chlorides': [0.61, 0.012],\n",
       " 'free_sulfur_dioxide': [72.0, 1.0],\n",
       " 'total_sulfur_dioxide': [289.0, 6.0],\n",
       " 'density': [1.00369, 0.99007],\n",
       " 'pH': [4.01, 2.74],\n",
       " 'sulphates': [2.0, 0.33],\n",
       " 'alcohol': [14.0, 8.4]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_serving_input_fn(my_params=params):\n",
    "    feature_names = my_params['feature_names']\n",
    "    receiver_tensors = {}\n",
    "    input_feature = 'x'\n",
    "    for names in feature_names:\n",
    "        max_val = my_params[names][0]\n",
    "        min_val = my_params[names][1]\n",
    "        dif = max_val - min_val \n",
    "        receiver_tensors[names] = (tf.keras.backend.placeholder(shape=[None,1], dtype=tf.float32, name=names) - min_val)/dif  \n",
    "    \n",
    "    \n",
    "    features = {\n",
    "        input_feature: tf.concat([\n",
    "            receiver_tensors['fixed_acidity'],\n",
    "            receiver_tensors['volatile_acidity'],\n",
    "            receiver_tensors['citric_acid'],\n",
    "            receiver_tensors['residual_sugar'],\n",
    "            receiver_tensors['chlorides'],\n",
    "            receiver_tensors['free_sulfur_dioxide'],\n",
    "            receiver_tensors['total_sulfur_dioxide'],\n",
    "            receiver_tensors['density'],\n",
    "            receiver_tensors['pH'],\n",
    "            receiver_tensors['sulphates'],\n",
    "            receiver_tensors['alcohol'],\n",
    "        ], axis=1) \n",
    "    }\n",
    "    return  tf.estimator.export.ServingInputReceiver(receiver_tensors=receiver_tensors, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_dir = '../model/models/wine_model' #rename long number with number of version\n",
    "\n",
    "serving_input_fn = my_serving_input_fn(my_params=params)\n",
    "\n",
    "export_path = my_second_estimator.export_saved_model(\n",
    "    export_dir_base=export_dir, serving_input_receiver_fn=lambda:my_serving_input_fn(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run -p 8501:8501 --mount type=bind,source=/home/jpriest/Desktop/intro_to_estimators/model/models/wine_model/,target=/models/wine_model -e MODEL_NAME=wine_model -t tensorflow/serving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -d @/home/jpriest/Desktop/instance.json -X POST http://localhost:8501/v1/models/wine_model:predict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
